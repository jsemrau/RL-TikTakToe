{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "import os\n",
    "#custom\n",
    "from cnn_utils import *\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "    \n",
    "from dotenv import load_dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Game state\n",
    "\n",
    "1. The game starts on an empty grid\n",
    "2. Every agent adds one X (1) or O (-1) when its their turn\n",
    "3. The game ends when one player has three games in a row, column, or diagonally, or\n",
    "4. there are no more open fields\n",
    "\n",
    "Rewards\n",
    "\n",
    "1. if one player wins, this player gets the reward\n",
    "2. if it is a draw, both players get no points.\n",
    "   I.e., a loss = a draw\n",
    "\n",
    "Goal\n",
    "Win the game. Not more sophisticated strategies\n",
    "\n",
    "Policy\n",
    "Here a policy is a rule that tells the player what move to make for every state of the game. \n",
    "For each considered policy an estimate of its winning probability would be obtained by playing \n",
    "some number of games against the opponent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TicTacToe import Player as p\n",
    "from TicTacToe import HumanPlayer as hp\n",
    "from TicTacToe import State as s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "| x | x | o | \n",
      "-------------\n",
      "| x | o | o | \n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "p1 = p.Player(\"p1\")\n",
    "p2 = p.Player(\"p2\")\n",
    "\n",
    "st = s.State(p1, p2)\n",
    "print(\"Playing a few training rounds...\")\n",
    "st.play(2)\n",
    "#store the success policies of each agent\n",
    "p1.savePolicy()\n",
    "p2.savePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with human\n",
    "#p1 = p.Player(\"computer\", exp_rate=0)\n",
    "#p1.loadPolicy(\"policy_p1\")\n",
    "\n",
    "#p2 = hp.HumanPlayer(\"human\")\n",
    "\n",
    "#st = s.State(p1, p2)\n",
    "#st.play2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
